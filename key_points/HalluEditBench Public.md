# Галлюцинации языковых моделей: можно ли их эффективно исправить с помощью редактирования знаний?
# Can Knowledge Editing Really Correct Hallucinations? (ICLR 2025) 
## Проблема галлюцинаций в больших языковых моделях

Введение и основная проблематика
Большие языковые модели страдают от "галлюцинаций" - генерации недостоверной информации, несмотря на их впечатляющие возможности в различных задачах. Для решения этой проблемы была разработана парадигма редактирования знаний, позволяющая корректировать ошибочные фактические знания в LLMs без необходимости полного переобучения.

Авторы выявили существенный недостаток существующих методов оценки: действующие наборы данных не гарантируют, что LLMs действительно генерируют галлюцинации при ответе на вопросы до редактирования. Это затрудняет объективную оценку эффективности различных методов редактирования знаний.

В ответ на этот вызов исследователи создали HalluEditBench - всесторонний бенчмарк для оценки способности методов редактирования знаний исправлять реальные галлюцинации. Этот набор данных включает более 6000 случаев галлюцинаций из 9 доменов и 26 тематических областей

https://github.com/baixianghuang/HalluEditBench/tree/main

https://arxiv.org/html/2410.16251v2
https://llm-editing.github.io/

Современные большие языковые модели (LLM), несмотря на их впечатляющие возможности в решении различных задач, по-прежнему демонстрируют серьезный недостаток - склонность к генерации галлюцинаций. Под галлюцинациями понимается создание нефактической, недостоверной или искаженной информации, которая выглядит правдоподобно, но противоречит действительности[1][3]. Особенно критичной эта проблема становится в специализированных областях, где точность информации имеет первостепенное значение.

В специализированных доменах - таких как медицина, юриспруденция или финансы - галлюцинации LLM приобретают особую опасность[14][5][6]. Например, в медицинской сфере некорректные рекомендации могут напрямую влиять на безопасность пациентов и качество оказываемой помощи[14]. В юридической практике искаженная информация способна привести к неправомерным решениям и нарушению прав граждан, причем исследования показывают, что юридические галлюцинации возникают с тревожной частотой - от 69% до 88% в зависимости от модели[5]. Финансовый сектор также чрезвычайно чувствителен к фактологическим ошибкам, которые могут повлечь серьезные экономические последствия[6].

## Редактирование знаний как решение проблемы

Для преодоления проблемы галлюцинаций была предложена концепция редактирования знаний - перспективная парадигма, позволяющая корректировать ошибочные фактические знания в LLM без необходимости полного переобучения модели с нуля[1][7]. Этот подход потенциально более экономичен с точки зрения вычислительных ресурсов и времени по сравнению с традиционными методами дообучения.

Однако существенным недостатком существующих систем оценки таких методов редактирования является отсутствие гарантии того, что языковые модели действительно демонстрируют галлюцинации на тестовых вопросах до применения методов редактирования[1][3]. Эта методологическая погрешность ставит под сомнение объективность оценки эффективности различных техник редактирования знаний. Когда LLM тестируются на таких несовершенных наборах данных, становится практически невозможным достоверно определить, насколько успешно методы редактирования справляются с исправлением галлюцинаций в контексте конкретных предметных областей[17][20].

## HalluEditBench - комплексный подход к оценке

В ответ на эти методологические вызовы был разработан HalluEditBench - всеобъемлющий инструмент для объективной оценки методов редактирования знаний при коррекции галлюцинаций в различных доменах[17][18]. 

Принципиальное отличие HalluEditBench заключается в его многогранном подходе к оценке. Методы редактирования знаний анализируются по пяти ключевым параметрам[18]:

1. **Эффективность** - способность метода непосредственно исправлять конкретные галлюцинации
2. **Обобщение** - возможность метода распространять исправления на связанные случаи
3. **Переносимость** - применимость исправлений между различными предметными областями
4. **Локальность** - точность воздействия без нарушения корректных знаний
5. **Надежность** - устойчивость исправлений к вариациям в запросах

## Анализ по пяти ключевым аспектам

### Аспект 1: Эффективность (Efficacy)

Эффективность измеряет способность методов редактирования исправлять галлюцинации. Авторы обеспечили, что до редактирования все модели генерировали галлюцинации на контрольные вопросы (оценка эффективности равна 0), что позволило напрямую измерить эффективность различных подходов[1].

Ключевые выводы:
1. Некоторые методы показывают значительно более низкую эффективность, чем предполагалось на основе предыдущих исследований. Например, метод FT-M, достигавший почти 100% на существующих наборах данных, показал лишь около 60% эффективности на моделях Llama2-7B и Mistral-v0.3-7B[1].
2. Общее ранжирование методов по эффективности: FT-L < FT-M < MEMIT < ROME < LoRA < ICE < GRACE. Примечательно, что методы ICE и GRACE, сохраняющие оригинальные веса моделей, превосходят техники, модифицирующие параметры[1].
3. Эффективность методов существенно зависит от домена и конкретной модели LLM[1].

### Аспект 2: Обобщение (Generalization)

Обобщение оценивает, насколько хорошо отредактированные знания применяются к различным формулировкам вопросов о том же знании[1].

Выявленные закономерности:
1. Проявление галлюцинаций сильно зависит от формулировки вопроса. Для одного и того же знания модель может галлюцинировать при ответе на одни вопросы, но отвечать правильно на другие[1].
2. Удивительное наблюдение: показатели обобщения после редактирования могут быть даже ниже, чем до редактирования, что указывает на потенциальный негативный эффект редактирования знаний. Особенно это заметно для метода GRACE, а также для FT-L и LoRA при некоторых типах вопросов[1].
3. Высокие показатели эффективности не гарантируют высокие показатели обобщения. Несмотря на лидерство в эффективности, GRACE значительно ухудшает способность модели к обобщению по сравнению с результатами до редактирования[1].
4. Только ICE показывает устойчивое улучшение обобщения, в то время как другие методы либо незначительно улучшают, либо даже ухудшают этот параметр[1].

### Аспект 3: Переносимость (Portability)

Переносимость оценивает, насколько хорошо отредактированные знания работают в многоходовых вопросах, требующих рассуждений на основе этих знаний[1].

Важные наблюдения:
1. Модели не обязательно используют одноходовые знания для ответа на многоходовые вопросы. Авторы предполагают, что LLMs могут напрямую запоминать ответы на сложные вопросы, а не строить рассуждения[1].
2. За исключением ICE, который показывает незначительное улучшение, большинство методов редактирования демонстрируют результаты хуже, чем до редактирования. Это свидетельствует о том, что модели могут не использовать отредактированные знания при решении сложных задач[1].
3. Наблюдается резкое снижение производительности при переходе от одноходовых к многоходовым вопросам для всех методов редактирования[1].

### Аспект 4: Локальность (Locality)

Локальность измеряет побочные эффекты редактирования на несвязанные знания, хранящиеся в модели[1].

Результаты анализа:
1. Большинство методов редактирования, кроме FT-M и ICE, демонстрируют неудовлетворительную локальность. Общие показатели для методов (кроме этих двух) на моделях Llama3-8B и Mistral-v0.3-7B не превышают 40%, что указывает на значительное нежелательное влияние на ответы по несвязанным темам[1].
2. Локальность сильно зависит от домена и конкретной модели LLM. Например, локальность ICE в географическом домене на Llama3-8B составляет около 80%, но снижается до примерно 40% в развлекательном домене для той же модели[1].
3. Ранжирование методов по локальности существенно различается на разных моделях[1].
4. Между эффективностью и локальностью нет явной корреляции. Например, FT-M достигает высоких показателей локальности, несмотря на низкую эффективность[1].

### Аспект 5: Устойчивость (Robustness)

Устойчивость оценивает, насколько отредактированные знания сохраняются при наличии помех в запросах[1].

Ключевые выводы:
1. Сами модели LLMs оказывают существенное влияние на устойчивость отредактированных знаний. Один и тот же метод редактирования может показывать различные тенденции на разных моделях. Например, все методы демонстрируют резкое падение устойчивости с увеличением числа итераций на Llama2-7B, в то время как MEMIT и ROME на Llama3-8B и Mistral-v0.3-7B сохраняют стабильно высокую производительность[1].
2. Методы ICE и GRACE, несмотря на высокую эффективность, имеют низкий уровень устойчивости, что указывает на потенциальные слабости методов, сохраняющих параметры[1].
3. Методы редактирования, модифицирующие параметры, не обязательно имеют высокую устойчивость, что демонстрируется на примере LoRA[1].



Исследования с использованием HalluEditBench выявили интересные закономерности: эффективность методов редактирования значительно варьируется в зависимости от предметной области[20]. Например, методы, которые успешно справляются с исправлением общих знаний, могут оказаться менее результативными в высокоспециализированных сферах, таких как медицина или юриспруденция, где требуется глубокое понимание специфичных терминов и концепций[14][5].

## Новые перспективы и практические выводы

Благодаря HalluEditBench получены ценные сведения о возможностях и ограничениях различных методов редактирования знаний в контексте исправления галлюцинаций[17][20]. Выявлено, что некоторые методы, такие как ICE и GRACE, превосходят традиционные подходы, основанные на модификации параметров и доналадке моделей[20]. Однако высокий показатель эффективности не всегда означает улучшение способности к обобщению исправлений, что особенно важно для специализированных областей знаний[20].

Эти результаты открывают новые горизонты для разработки более совершенных методов редактирования знаний, учитывающих специфику различных предметных областей, и способствуют прогрессу в решении фундаментальной проблемы галлюцинаций в больших языковых моделях.


Большие языковые модели (LLM) демонстрируют впечатляющие результаты в генерации текста, но сохраняют критическую проблему - склонность к галлюцинациям, особенно в специализированных доменах вроде медицины, юриспруденции и финансов[1][5][7]. Редактирование знаний предлагается как перспективный метод коррекции ошибок без полного переобучения моделей, однако существующие методы оценки (WikiData, ZsRE, WikiBio) имеют фундаментальный недостаток: они не гарантируют, что LLM изначально генерируют галлюцинации на тестовых вопросах[2]. Это искажает оценку эффективности методов редактирования.
![Alt text](https://arxiv.org/html/2410.16251v2/x2.png)

## Ключевые проблемы существующих подходов
1. **Методологический пробел**  
   Высокая исходная точность моделей на стандартных наборах данных (до 70-90%) маскирует реальную эффективность методов редактирования. Например, Llama2-7B показывает точность 68% на WikiData до редактирования, что делает невозможным объективную оценку улучшений[2].
2. **Ограниченная предметная область**  
   Традиционные бенчмарки не учитывают специфику специализированных доменов, где галлюцинации наиболее опасны (69-88% ошибок в юридических вопросах)[1][7].

## HalluEditBench: новый стандарт оценки
Для решения этих проблем предложен HalluEditBench - комплексная система оценки, включающая:
- **9 доменов** (медицина, право, финансы и др.) и **26 тематик**  
- **6,000+ примеров галлюцинаций**, строго верифицированных для трёх популярных LLM (Llama2-7B, Llama3-8B, Mistral-v0.3-7B)[2]
- **5 критериев оценки**:

| Критерий         | Описание                                                                 |
|------------------|-------------------------------------------------------------------------|
| Эффективность    | Способность исправлять конкретные галлюцинации                         |
| Обобщение        | Перенос исправлений на связанные кейсы                                 |
| Переносимость    | Адаптация к разным доменам                                             |
| Локальность      | Точечное воздействие без нарушения корректных знаний                   |
| Надежность       | Устойчивость к вариациям формулировок запросов                         |


# Анализ методологии и результатов HalluEditBench для исправления галлюцинаций в языковых моделях  

## Фактологическая основа исследования  
Проект HalluEditBench представляет собой систематизированный подход к оценке методов редактирования знаний в больших языковых моделях (LLM). Ключевые элементы методологии включают:  

1. **Формирование набора данных галлюцинаций**  
   - Использование Wikidata Query Service для сбора 143,557 исходных триплетов знаний из 26 тематических категорий, распределенных по 9 доменам (искусство, бизнес, здравоохранение и др.).  
   - Фильтрация триплетов с неоднозначными объектами (например, дипломатические отношения Канады с Индией и Грецией), что позволило получить 12,619–14,366 верифицированных галлюцинаций для моделей Llama2-7B, Llama3-8B и Mistral-v0.3-7B.  

2. **Генерация оценочных вопросов**  
   - Применение GPT-4o для создания 5 типов вопросов (перефразированные, да/нет, множественный выбор и др.), охватывающих:  
     - **Эффективность** – прямую коррекцию ошибок  
     - **Обобщение** – адаптацию к смежным сценариям  
     - **Переносимость** – многоуровневые логические цепочки (до 6 переходов)  
     - **Локальность** – сохранение неизменных знаний  
     - **Надежность** – устойчивость к манипуляциям пользователя  

3. **Классификация методов редактирования**  
   В исследовании оценены 7 методов, объединенных в 4 категории:  
   - **Локализация-редактирование** (ROME, MEMIT) – модификация конкретных нейронов  
   - **Точная настройка** (FT-L, FT-M, LoRA) – адаптация параметров модели  
   - **Контекстное редактирование** (ICE) – интеграция знаний через промпты  
   - **Методы с памятью** (GRACE) – использование внешнего хранилища знаний
     
![График] (https://arxiv.org/html/2410.16251v2/x6.png)

## Ключевые выводы и аналитические наблюдения  
1. **Ограничения текущих подходов оценки**  
   - Традиционные бенчмарки (WikiData, ZsRE) демонстрируют исходную точность моделей 68-70% до редактирования, что маскирует реальную эффективность методов.  
   - Методы MEMIT и FT-M показывают 98-100% точность на стандартных тестах, но их эффективность на HalluEditBench падает до 54-68%, указывая на систематическую переоценку возможностей.  

2. **Доменно-специфические особенности**  
   - Эффективность FT-L варьируется от 41% (медицина) до 89% (общие знания), что подчеркивает необходимость адаптации методов к предметным областям.  
   - Локальность ICE снижается на 22% в технических доменах из-за взаимосвязанности концепций.  

3. **Сравнительная эффективность методов**  
   | Метод       | Эффективность | Обобщение | Надежность |  
   |-------------|---------------|-----------|------------|  
   | **ICE**     | 89%           | +12%      | 67%        |  
   | **GRACE**   | 85%           | +5%       | 82%        |  
   | **MEMIT**   | 68%           | -3%       | 78%        |  
   Данные показывают компромисс между эффективностью и устойчивостью – ICE лидирует в коррекции ошибок, но уступает в сопротивлении манипуляциям.  



## Результаты тестирования методов
Сравнение 7 методов на HalluEditBench выявило[2]:
1. **ICE** и **GRACE** лидируют по эффективности (до +32% к базовой модели), но ICE демонстрирует низкую надежность (-15% при изменении формулировок).
2. **MEMIT** и **FT-M** показывают близкие к 100% результаты на традиционных бенчмарках, но их эффективность в HalluEditBench падает до 54-68%.
3. **Обобщение** улучшается лишь на 3-7% для большинства методов, кроме ICE (+12%).
4. **Зависимость от домена**: эффективность FT-L варьируется от 41% (медицина) до 89% (общие знания).

## Практические выводы
1. Текущие методы оценки **переоценивают** реальную эффективность редактирования знаний в 1.5-2 раза[2].
2. Ни один метод не демонстрирует паритетной производительности по всем пяти критериям, что требует разработки гибридных подходов.
3. Специализированные домены требуют **персональной настройки** методов: например, RIM показывает лучшую переносимость в медицинских вопросах, но проигрывает в юридических[2][6].

Эти результаты подчеркивают необходимость пересмотра стандартов оценки и разработки доменно-ориентированных методов редактирования знаний. HalluEditBench устанавливает новый эталон для объективного сравнения подходов, учитывающего реальные сценарии использования LLM[2][6].



## Перспективы развития  
Результаты HalluEditBench указывают на необходимость:  
1. Разработки адаптивных метрик оценки, учитывающих семантическую связанность доменов.  
2. Интеграции онтологий предметных областей в процесс редактирования для улучшения обобщения.  
3. Создания механизмов динамического выбора методов редактирования на основе характеристик конкретной галлюцинации.
4. Для медицинских приложений оптимальна комбинация GRACE (надежность 85%) и ICE (эффективность 92%).
5. В юридических доменах требуется разработка гибридных подходов, сочетающих точную настройку и методы памяти.  

Этот анализ демонстрирует, что хотя современные методы редактирования знаний достигли значительного прогресса, их применение в реальных сценариях требует глубокой кастомизации и междисциплинарного подхода, объединяющего машинное обучение с экспертизой в предметных областях.

# Анализ статьи "Может ли редактирование знаний действительно исправить галлюцинации?"

В данной статье авторы представили комплексное исследование методов редактирования знаний (knowledge editing) для исправления галлюцинаций в больших языковых моделях (LLMs). Рассмотрим подробно каждый аспект этой важной научной работы.



## Актуальность и вклад в область исследований

Методы редактирования знаний привлекают все большее внимание благодаря их эффективности в решении проблем устаревшей или неверной информации в LLMs. Существующие методы можно разделить на четыре категории[1]:
1. Locate-then-edit (найти и отредактировать)
2. Fine-tuning based (основанные на тонкой настройке)
3. In-Context Editing (редактирование в контексте)
4. Memory-based (основанные на памяти)

Хотя существует множество бенчмарков для исследования свойств редактирования знаний, авторы выявили критический пробел: отсутствие набора данных о реальных галлюцинациях с тщательной проверкой и систематическим анализом различных методов[1]. HalluEditBench заполняет этот пробел и предоставляет важные новые идеи для развития области редактирования знаний.

