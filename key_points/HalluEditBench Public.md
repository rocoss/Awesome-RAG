# Галлюцинации языковых моделей: можно ли их эффективно исправить с помощью редактирования знаний?
# Can Knowledge Editing Really Correct Hallucinations? (ICLR 2025) 
## Проблема галлюцинаций в больших языковых моделях


https://github.com/baixianghuang/HalluEditBench/tree/main

https://arxiv.org/html/2410.16251v2
https://llm-editing.github.io/

Современные большие языковые модели (LLM), несмотря на их впечатляющие возможности в решении различных задач, по-прежнему демонстрируют серьезный недостаток - склонность к генерации галлюцинаций. Под галлюцинациями понимается создание нефактической, недостоверной или искаженной информации, которая выглядит правдоподобно, но противоречит действительности[1][3]. Особенно критичной эта проблема становится в специализированных областях, где точность информации имеет первостепенное значение.

В специализированных доменах - таких как медицина, юриспруденция или финансы - галлюцинации LLM приобретают особую опасность[14][5][6]. Например, в медицинской сфере некорректные рекомендации могут напрямую влиять на безопасность пациентов и качество оказываемой помощи[14]. В юридической практике искаженная информация способна привести к неправомерным решениям и нарушению прав граждан, причем исследования показывают, что юридические галлюцинации возникают с тревожной частотой - от 69% до 88% в зависимости от модели[5]. Финансовый сектор также чрезвычайно чувствителен к фактологическим ошибкам, которые могут повлечь серьезные экономические последствия[6].

## Редактирование знаний как решение проблемы

Для преодоления проблемы галлюцинаций была предложена концепция редактирования знаний - перспективная парадигма, позволяющая корректировать ошибочные фактические знания в LLM без необходимости полного переобучения модели с нуля[1][7]. Этот подход потенциально более экономичен с точки зрения вычислительных ресурсов и времени по сравнению с традиционными методами дообучения.

Однако существенным недостатком существующих систем оценки таких методов редактирования является отсутствие гарантии того, что языковые модели действительно демонстрируют галлюцинации на тестовых вопросах до применения методов редактирования[1][3]. Эта методологическая погрешность ставит под сомнение объективность оценки эффективности различных техник редактирования знаний. Когда LLM тестируются на таких несовершенных наборах данных, становится практически невозможным достоверно определить, насколько успешно методы редактирования справляются с исправлением галлюцинаций в контексте конкретных предметных областей[17][20].

## HalluEditBench - комплексный подход к оценке

В ответ на эти методологические вызовы был разработан HalluEditBench - всеобъемлющий инструмент для объективной оценки методов редактирования знаний при коррекции галлюцинаций в различных доменах[17][18]. В основе этого инструмента лежит тщательно структурированный набор данных, охватывающий 9 различных предметных областей и 26 тематических категорий, содержащий более 6000 примеров галлюцинаций[1][7].

Принципиальное отличие HalluEditBench заключается в его многогранном подходе к оценке. Методы редактирования знаний анализируются по пяти ключевым параметрам[18]:

1. **Эффективность** - способность метода непосредственно исправлять конкретные галлюцинации
2. **Обобщение** - возможность метода распространять исправления на связанные случаи
3. **Переносимость** - применимость исправлений между различными предметными областями
4. **Локальность** - точность воздействия без нарушения корректных знаний
5. **Надежность** - устойчивость исправлений к вариациям в запросах

Исследования с использованием HalluEditBench выявили интересные закономерности: эффективность методов редактирования значительно варьируется в зависимости от предметной области[20]. Например, методы, которые успешно справляются с исправлением общих знаний, могут оказаться менее результативными в высокоспециализированных сферах, таких как медицина или юриспруденция, где требуется глубокое понимание специфичных терминов и концепций[14][5].

## Новые перспективы и практические выводы

Благодаря HalluEditBench получены ценные сведения о возможностях и ограничениях различных методов редактирования знаний в контексте исправления галлюцинаций[17][20]. Выявлено, что некоторые методы, такие как ICE и GRACE, превосходят традиционные подходы, основанные на модификации параметров и доналадке моделей[20]. Однако высокий показатель эффективности не всегда означает улучшение способности к обобщению исправлений, что особенно важно для специализированных областей знаний[20].

Эти результаты открывают новые горизонты для разработки более совершенных методов редактирования знаний, учитывающих специфику различных предметных областей, и способствуют прогрессу в решении фундаментальной проблемы галлюцинаций в больших языковых моделях.


Большие языковые модели (LLM) демонстрируют впечатляющие результаты в генерации текста, но сохраняют критическую проблему - склонность к галлюцинациям, особенно в специализированных доменах вроде медицины, юриспруденции и финансов[1][5][7]. Редактирование знаний предлагается как перспективный метод коррекции ошибок без полного переобучения моделей, однако существующие методы оценки (WikiData, ZsRE, WikiBio) имеют фундаментальный недостаток: они не гарантируют, что LLM изначально генерируют галлюцинации на тестовых вопросах[2]. Это искажает оценку эффективности методов редактирования.
![Alt text]((https://arxiv.org/html/2410.16251v2/x2.png)]

## Ключевые проблемы существующих подходов
1. **Методологический пробел**  
   Высокая исходная точность моделей на стандартных наборах данных (до 70-90%) маскирует реальную эффективность методов редактирования. Например, Llama2-7B показывает точность 68% на WikiData до редактирования, что делает невозможным объективную оценку улучшений[2].
2. **Ограниченная предметная область**  
   Традиционные бенчмарки не учитывают специфику специализированных доменов, где галлюцинации наиболее опасны (69-88% ошибок в юридических вопросах)[1][7].

## HalluEditBench: новый стандарт оценки
Для решения этих проблем предложен HalluEditBench - комплексная система оценки, включающая:
- **9 доменов** (медицина, право, финансы и др.) и **26 тематик**  
- **6,000+ примеров галлюцинаций**, строго верифицированных для трёх популярных LLM (Llama2-7B, Llama3-8B, Mistral-v0.3-7B)[2]
- **5 критериев оценки**:

| Критерий         | Описание                                                                 |
|------------------|-------------------------------------------------------------------------|
| Эффективность    | Способность исправлять конкретные галлюцинации                         |
| Обобщение        | Перенос исправлений на связанные кейсы                                 |
| Переносимость    | Адаптация к разным доменам                                             |
| Локальность      | Точечное воздействие без нарушения корректных знаний                   |
| Надежность       | Устойчивость к вариациям формулировок запросов                         |

## Результаты тестирования методов
Сравнение 7 методов на HalluEditBench выявило[2]:
1. **ICE** и **GRACE** лидируют по эффективности (до +32% к базовой модели), но ICE демонстрирует низкую надежность (-15% при изменении формулировок).
2. **MEMIT** и **FT-M** показывают близкие к 100% результаты на традиционных бенчмарках, но их эффективность в HalluEditBench падает до 54-68%.
3. **Обобщение** улучшается лишь на 3-7% для большинства методов, кроме ICE (+12%).
4. **Зависимость от домена**: эффективность FT-L варьируется от 41% (медицина) до 89% (общие знания).

## Практические выводы
1. Текущие методы оценки **переоценивают** реальную эффективность редактирования знаний в 1.5-2 раза[2].
2. Ни один метод не демонстрирует паритетной производительности по всем пяти критериям, что требует разработки гибридных подходов.
3. Специализированные домены требуют **персональной настройки** методов: например, RIM показывает лучшую переносимость в медицинских вопросах, но проигрывает в юридических[2][6].

Эти результаты подчеркивают необходимость пересмотра стандартов оценки и разработки доменно-ориентированных методов редактирования знаний. HalluEditBench устанавливает новый эталон для объективного сравнения подходов, учитывающего реальные сценарии использования LLM[2][6].


