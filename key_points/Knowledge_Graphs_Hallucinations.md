# Методы снижения галлюцинаций в больших языковых моделях с использованием графов знаний: анализ подходов
# Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective
!(тык)(https://arxiv.org/html/2411.14258v1)

Данный отчет представляет подробный анализ методов, используемых для борьбы с галлюцинациями в больших языковых моделях (LLM) с помощью графов знаний (KG). В основе анализа лежит структура из 8 ключевых направлений исследований в этой области.

## 1. Введение: суть проблемы галлюцинаций в языковых моделях

Большие языковые модели (LLM) произвели революцию в области обработки естественного языка, однако они сталкиваются с серьезной проблемой: галлюцинации - когда модели генерируют правдоподобные, но фактически неверные ответы. Это подрывает доверие и ограничивает применимость LLM в различных областях, от медицины до юриспруденции.

Графы знаний (KG) представляют собой структурированные коллекции взаимосвязанных фактов, представленных как сущности (узлы) и их отношения (ребра). В последних исследованиях KG используются для предоставления контекста, который может заполнить пробелы в понимании LLM определенных тем, предлагая перспективный подход для снижения галлюцинаций[1].

Как отмечается в статье, галлюцинации многогранны и различаются по типам: галлюцинации относительно фактических знаний, самопротиворечия, галлюцинации относительно инструкций в запросе или заданного контекста[1]. Хотя в некоторых случаях (например, при мозговом штурме или создании художественных работ) галлюцинации могут быть полезны, они являются ограничивающим фактором для контекстов, где фактическая точность является приоритетом.

## 2. Доступные ресурсы для оценки галлюцинаций

В связи с бурным развитием LLM в последние годы оценка галлюцинаций стала все более важной, что привело к увеличению количества специализированных наборов данных и бенчмарков.
![изображение](https://ars.els-cdn.com/content/image/1-s2.0-S1570826824000301-gr1_lrg.jpg)

### 2.1. Ключевые бенчмарки и метрики

Были предложены различные эталонные тесты и метрики для оценки галлюцинаций, такие как:

- **CHAIR** - оценивает галлюцинации объектов в подписях к изображениям[3]
- **POPE** - включает 500 изображений MSCOCO для ответов "Да/Нет" об объектах[3]
- **MME** - охватывает 14 подзадач, включая наличие объектов, их количество, положение и цвет[3]
- **CIEM** - автоматически генерирует пары "вопрос-ответ" с помощью LLM для оценки галлюцинаций[3]
- **GAVIE** - использует GPT-4 для оценки релевантности выходных данных MLLM инструкциям[3]
- **AMBER** - оценивает как генеративные, так и дискриминативные задачи MLLM на галлюцинациях наличия объектов, атрибутов и отношений[3]

### 2.2. Основные наборы данных

Большинство наборов данных, представленных в статье, являются многодоменными и охватывают различные области, такие как право, политика, медицина, наука и технологии, искусство, финансы и другие. Среди них:

- **Shroom SemEval 2024** - для обнаружения галлюцинаций в машинном переводе, перефразировании и моделировании определений[1]
- **MuShroom SemEval 2025** - многоязычный набор данных для обнаружения галлюцинаций на уровне отрезков текста[1]
- **MedHalt** - медицинский набор данных для оценки галлюцинаций в рассуждениях и информационном поиске[1]
- **HaluEval** - для обнаружения галлюцинаций в вопросно-ответных системах, суммаризации и диалогах[1]
- **TruthfulQA** - многодоменный набор данных для оценки галлюцинаций в QA[1]

Большинство из этих наборов данных сфокусированы на выявлении галлюцинаций на уровне ответа, а не на более мелком уровне предложений или отрезков текста. Кроме того, все наборы данных, за исключением MuShroom SemEval 2025, доступны только на английском языке, что ограничивает многоязычную оценку[1].

## 3. Возможность снижения галлюцинаций

Исследования показывают, что LLM не будут на 100% свободны от риска галлюцинаций, однако доступ к внешним знаниям может эффективно снизить их проявление[1][7]. Это приводит к двум ключевым требованиям для повышения надежности систем LLM:

### 3.1. Интерпретируемость вывода

Необходимо обеспечить интерпретируемость результатов, позволяя конечному пользователю анализировать и проверять вывод модели из-за её склонности к галлюцинациям[1].

### 3.2. Использование внешних источников знаний

Обусловливание LLM надежным внешним источником знаний для смягчения галлюцинаций. Графы знаний в этом случае особенно полезны при условии, что содержащиеся в них тройки фактически верны по отношению к запросу пользователя[1].

Если LLM эффективно использует тройки графа знаний, то её вывод может быть отображен обратно на граф, что позволяет перепроверить информацию и убедиться в её достоверности[1][7].
![изображение](https://ars.els-cdn.com/content/image/1-s2.0-S1570826824000301-gr2_lrg.jpg)
## 4. Методы обнаружения галлюцинаций с использованием графов знаний

Обнаружение галлюцинаций - это задача определения, содержит ли текст, сгенерированный LLM, какие-либо формы галлюцинаций. Эта задача сложна из-за многогранного характера проблемы.

### 4.1. GraphEval

Представляет собой двухэтапный метод для обнаружения и смягчения галлюцинаций относительно заданного текстового контекста[4][12]. 

- На первом этапе извлекаются атомарные утверждения из вывода LLM в виде подграфа через подсказки LLM
- Затем проверяется соответствие каждой тройки данному текстовому контексту
- Метод позволяет идентифицировать конкретные тройки в графе знаний, склонные к галлюцинациям[4]

### 4.2. Методы на основе сравнения подграфов

Метод, описанный в работе[5], извлекает подграфы знаний между исходным и сгенерированным текстами на основе именованных сущностей (организаций, мест, людей и т. д.) для последующего сравнения согласованности между двумя графами[1].

- Преобразование проблемы в задачу выравнивания графа знаний
- Классификация галлюцинаций как внутренних или внешних
- Достигнут показатель F1 0,889 для обнаружения галлюцинаций и 0,825 для классификации типов галлюцинаций[5]

### 4.3. KGR (Knowledge Graph-based Retrofitting)

Выполняет обнаружение галлюцинаций через систему модулей для извлечения утверждений, выбора фактов и проверки[7].

- Автономный процесс проверки и уточнения знаний без дополнительных ручных усилий
- Использует LLM для извлечения, выбора, проверки и ретрофита фактических утверждений[7]

### 4.4. FLEEK

Система, направленная на проверку фактов с использованием доказательств из внешних источников знаний[6].

- Автоматически извлекает фактические утверждения из текста
- Собирает доказательства из внешних источников знаний
- Оценивает фактичность каждого утверждения
- Предлагает исправления для выявленных ошибок[6]

Общая тенденция в методах обнаружения галлюцинаций заключается в оценке утверждений на атомарном уровне, представляя их как структуры графа знаний, что повышает интерпретируемость результатов и позволяет точно указать проблемные места в тексте[1].

## 5. Методы интеграции знаний из графов знаний в большие языковые модели

Существует несколько подходов к интеграции знаний из графов знаний в LLM, которые можно разделить на несколько категорий в зависимости от этапа, на котором происходит интеграция.

### 5.1. Интеграция знаний на этапе предобучения

- **Фактически информированное предобучение** - включение троек графа знаний в процесс обучения модели[1][8]
- **ERNIE 3.0** - модель, предварительно обученная на трансформерах с использованием массивных неструктурированных текстов и графов знаний для изучения различных уровней знаний: лексической, синтаксической и семантической информации[8]
- **Методы на основе адаптеров** - кодируют знания из графов знаний, действуя как низкопараметрические дополнения к архитектуре LLM[1][10]

### 5.2. Интеграция знаний во время вывода

- **Подсказки (prompting)** - наивный метод интеграции внешних знаний через формирование ввода LLM через пары знаний и запросов. Используется в приложениях RAG для добавления полных документов или троек знаний[1]
- **Context-aware decoding** - метод, следующий контрастному распределению вывода, которое усиливает разницу между вероятностями вывода, когда модель используется с контекстом и без него[9]
- **KG-Adapter** - метод интеграции графов знаний на параметрическом уровне, основанный на эффективной настройке параметров (PEFT)[10]
  - Содержит Sub-word to Entity Hybrid Initialization (SEHI) и несколько слоев KG-Adapter
  - Позволяет LLM напрямую получать доступ к структурированным графам знаний
  - Требует всего 28M обучаемых параметров[10]

### 5.3. Интеграция знаний на этапе пост-генерации

- **KGR (Knowledge Graph-based Retrofitting)** - предлагает модернизацию фактичности вывода LLM путем обращения к внешнему графу знаний после генерации ответа[7]
- Методология следует 5-этапному конвейеру:
  1. Генерация ответа
  2. Извлечение утверждений
  3. Проверка по внешнему графу знаний
  4. Валидация утверждений
  5. Коррекция исходного ответа в соответствии с результатами проверки[7]

### 5.4. Многоязычные аспекты интеграции знаний

- Исследования показывают, что галлюцинации более вероятны в языках с меньшим количеством ресурсов[1]
- Языковые модели могут иметь непоследовательные представления знаний и различия между языками[1]
- Многоязычные графы знаний особенно полезны для языков с низким ресурсом, где данные обучения ограничены[1]

## 6. Ограничения графов знаний в решении проблемы галлюцинаций

Использование графов знаний для снижения галлюцинаций имеет ряд ограничений и требований к качеству данных.

### 6.1. Требования к качеству данных графов знаний

Модуль снижения галлюцинаций должен поддерживать графы знаний со следующими характеристиками:
1. **Полнота данных** - отсутствие пропущенных отношений и неоднозначностей[1]
2. **Точность данных** - актуальность и фактическая точность[1]
3. **Многоязычное покрытие** - разнообразное языковое представление[1]

### 6.2. Ограничения существующих графов знаний

- **Wikidata** в основном содержит европейские языки и имеет искаженную полноту данных с точки зрения гендера, социально-экономического положения, географических регионов и временной актуальности[1]
- **PubMedKG**, специализированный граф знаний для медицинской области, имеет около 2% неоднозначных имен авторов и примерно 90% успеха в разрешении неоднозначности медицинских сущностей[1]
- **Предметно-ориентированные графы знаний** (например, для предприятий или юридических кейсов) также имеют ограничения в зависимости от их исходных данных[1]

### 6.3. Баланс между источниками знаний

- **Generate-on-Graph (GoG)** - методология для балансировки информации из графа знаний и внутренних знаний LLM, использующая парадигму "думай-ищи-генерируй"[1]
- **GECKO** - полагается исключительно на генерацию текста на основе информации в графе знаний[1]

Понимание этих ограничений важно для правильного выбора и применения графов знаний в конкретных задачах снижения галлюцинаций.

## 7. Об оценке галлюцинаций

Оценка фактической точности LLM - сложная задача, особенно для генеративных моделей, где необходимо оценивать семантику вывода.

### 7.1. Подходы к оценке

- **Многовариантный выбор** - наборы данных, такие как MedHalt и TruthfulQA, моделируют оценку ответов как задачу выбора из предопределенного списка ответов[1]
- **Генеративная оценка** - более надежный подход, оценивающий семантику сгенерированного текста[1]
- **Нейронные метрики** - такие как BERTScore или BARTScore, оценивающие семантическое сходство между двумя текстами[1]
- **Модели текстуального следования** - для классификации того, следует ли гипотеза (вывод LLM) из данной предпосылки (фактических знаний) или противоречит ей[1]

### 7.2. Проблемы оценки

- Галлюцинации могут быть тонкими, и даже одно неправильное слово может привести к большому семантическому несоответствию[1]
- Методы, использующие вспомогательные LLM для оценки, сами подвержены галлюцинациям, особенно в условиях языков с низким ресурсом[1]
- Большинство методов оценивают текст целостно, не выявляя конкретные проблемные места[1]

### 7.3. Перспективные методы оценки

- **FactScore** - методология оценки фактичности, основанная на двух идеях:
  1. Разбить вывод LLM на атомарные факты
  2. Сравнить каждый атомарный факт с внешними знаниями[1]
- Этот подход может быть расширен для использования с графами знаний, сравнивая атомарные факты с тройками графа[1]

## 8. Выводы и направления будущих исследований

На основании проведенного анализа можно выделить следующие направления для будущих исследований в области снижения галлюцинаций с использованием графов знаний.

### 8.1. Перспективные направления исследований

1. **Эффективное и многоязычное создание и заполнение графов** - особенно важно для доступности технологий для неанглоязычных сообществ и незападных культур[1]
2. **Извлечение сущностей из текста** - критично для методов снижения галлюцинаций, основанных на извлечении атомарных фактов из текста[1]
3. **Извлечение графовых вложений и многоязычная привязка сущностей** - для поддержки языково-агностических вложений[1]
4. **Методы синергии графов знаний и LLM** - дальнейшее исследование оптимального взаимодействия между ними[1]

### 8.2. Необходимые ресурсы и инструменты

1. **Крупномасштабные наборы данных** с точными тройками графов знаний в качестве контекста, включающие обучающие, валидационные и тестовые выборки[1]
2. **Надежные методики оценки**, охватывающие многоязычность, многозадачность и множественные формулировки запросов[1]
3. **Методы обнаружения галлюцинаций** с детальным выделением проблемных участков текста[1]
4. **Методы интеграции знаний**, не зависящие от текстовых подсказок, в идеале в параметрически эффективной форме[1]

### 8.3. Интеграция различных подходов

Исследования по комбинированию фундаментально различных методов снижения галлюцинаций могут дать представление о том, как эти методы дополняют друг друга[1][19]. Особенно важно изучение синергии между методами на основе неопределенности LLM и методами на основе графов знаний для обнаружения галлюцинаций[1].

В целом, несмотря на значительный прогресс в этой области, снижение галлюцинаций в LLM с помощью графов знаний остается активной областью исследований с множеством открытых проблем и перспективных направлений развития.
